{% include base_path %}

{% if post.header.teaser %}
  {% capture teaser %}{{ post.header.teaser }}{% endcapture %}
{% else %}
  {% assign teaser = site.teaser %}
{% endif %}

{% if post.id %}
  {% assign title = post.title | markdownify | remove: "<p>" | remove: "</p>" %}
{% else %}
  {% assign title = post.title %}
{% endif %}

<div class="{{ include.type | default: "list" }}__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    {% if include.type == "grid" and teaser %}
      <div class="archive__item-teaser">
        <img src=
          {% if teaser contains "://" %}
            "{{ teaser }}"
          {% else %}
            "{{ teaser | prepend: "/images/" | prepend: base_path }}"
          {% endif %}
          alt="">
      </div>
    {% endif %}

    <h2 class="archive__item-title" itemprop="headline">
      {% if post.link %}
        <a href="{{ post.link }}">{{ title }}</a> <a href="{{ base_path }}{{ post.url }}" rel="permalink"><i class="fa fa-link" aria-hidden="true" title="permalink"></i><span class="sr-only">Permalink</span></a>
      {% else %}
        <a href="{{ base_path }}{{ post.url }}" rel="permalink">{{ title }}</a>
      {% endif %}
    </h2>
    
    {% if post.read_time %}
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> {% include read-time.html %}</p>
    {% endif %}

        {% if post.collection == 'teaching' %}
          <p> {{ post.type }}, <i>{{ post.venue }}</i>, {{ post.date | default: "1900-01-01" | date: "%Y" }} </p>
        {% elsif post.collection == 'publications' %}
          <p>Published in <i>{{ post.venue }}</i>, {{ post.date | default: "1900-01-01" | date: "%Y" }} </p>
        {% elsif post.date %}
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> {{ site.data.ui-text[site.locale].date_label | default: "Published:" }}</strong> <time datetime="{{ post.date | default: "1900-01-01" | date_to_xmlschema }}">{{ post.date | default: "1900-01-01" | date: "%B %d, %Y" }}</time></p>
        {% endif %}

    {% if post.excerpt and site.read_more != 'enabled' %}
    <p class="archive__item-excerpt" itemprop="description">{{ post.excerpt | markdownify }}</p>
    {% elsif post.excerpt and site.read_more == 'enabled' %}
    <p class="archive__item-excerpt" itemprop="description"><p>{{ post.excerpt | markdownify | remove: '<p>' | remove: '</p>' }}<strong><a href="{{ base_path }}{{ post.url }}" rel="permalink"> Read more</a></strong></p></p>
    {% endif %}
    
    {% if post.citation and post.paperurl %}
      <p>Recommended citation: {{ post.citation }} <a href="{{ post.paperurl }}"><u>{{ post.paperurl }}</u></a></p>
    {% elsif post.citation %}
      <p>Recommended citation: {{ post.citation }} </p>
    {% elsif post.paperurl %}
      <p>Download <a href=" {{ post.paperurl }} "><u>here</u></a></p>
    {% endif %}

  </article>
</div>




<table id="tbPublications" width="100%">
	<tbody>
    <tr>
	<td><center><img width="250" src="./pic/paper/sim2real.png"></center></td>
	<td>
		<font size="2">Policy Adaptation from Foundation Model Feedback,
		<br>
		<i><b>Yuying Ge</b>, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2023
		<br>
			[<a href='https://arxiv.org/abs/2212.07398' target="_blank"><b>paper</b></a>|<a href='https://geyuying.github.io/PAFF/' target="_blank"><b>project</b></a>]
		</td>
		
		
			<tr>
	<td><center><img width="200" src="./pic/paper/TVTS.jpeg"></center></td>
	<td>
		<font size="2">Learning Transferable Spatiotemporal Representations from Natural Script Knowledge,
		<br>
		<i>Ziyun Zeng*, <b>Yuying Ge*</b>, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2023
		<br>
			[<a href='https://arxiv.org/abs/2209.15280' target="_blank"><b>paper</b></a>|<a href='https://github.com/TencentARC/TVTS' target="_blank"><b>code</b></a>]
		</td>

		<tr>
	<td><center><img width="250" src="./pic/paper/MILES.jpg"></center></td>
	<td>
		<font size="2">MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval,
		<br>
		<i><b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie and Ping Luo</i>
		<br>
		European Conference on Computer Vision (<b>ECCV</b>) 2022
		<br>
			[<a href='https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136950685.pdf' target="_blank"><b>paper</b></a>|<a href='https://github.com/TencentARC/MCQ/blob/main/MILES.md' target="_blank"><b>code</b></a>]
		</td>

	</tr>

	<tr>
	<td><center><img width="250" src="./pic/paper/MCQ.jpg"></center></td>
	<td>
		<font size="2">Bridging Video-text Retrieval with Multiple Choice Questions,
		<br>
		<i><b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie and Ping Luo</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2022 (<b>oral</b>)
		<br>
		[<a href='https://openaccess.thecvf.com/content/CVPR2022/papers/Ge_Bridging_Video-Text_Retrieval_With_Multiple_Choice_Questions_CVPR_2022_paper.pdf' target="_blank"><b>paper</b></a>|<a href='https://github.com/TencentARC/MCQ' target="_blank"><b>code</b></a>|<a href='MCQ.html' target="_blank"><b>project</b></a>]
		</td>

	</tr>

		<tr>
	<td><center><img width="250" src="./pic/paper/metadance.jpg"></center></td>
	<td>
		<font size="2">MetaDance: Few-shot Dancing Video Retargeting via Temporal-aware Meta-learning,
		<br>
		<i><b>Yuying Ge</b>, Yibing Song, Ruimao Zhang and Ping Luo</i>
		<br>
		arXiv preprint, 2022
		<br>
		[<a href='https://arxiv.org/abs/2201.04851' target="_blank"><b>paper</b></a>|<a href='https://github.com/geyuying/MetaDance' target="_blank"><b>demo</b></a>]
		</td>

	</tr>
	<tr>
	<td><center><img width="250" src="./pic/paper/metacloth.jpg"></center></td>
	<td>
		<font size="2">MetaCloth: Learning Unseen Tasks of Dense Fashion Landmark Detection from a Few Samples,
		<br>
		<i><b>Yuying Ge</b>, Ruimao Zhang, and Ping Luo</i>
		<br>
		IEEE Transactions on Image Processing (<b>TIP</b>) 2021
		<br>
		[<a href='https://arxiv.org/abs/2112.02763' target="_blank"><b>paper</b></a>]
		</td>

	</tr>

	<tr>
	<td><center><img width="250" src="./pic/paper/cvpr2021_pfafn.png"></center></td>
	<td>
		<font size="2">Parser-Free Virtual Try-on via Distilling Appearance Flows,
		<br>
		<i><b>Yuying Ge</b>, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021
		<br>
		[<a href='https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_Parser-Free_Virtual_Try-On_via_Distilling_Appearance_Flows_CVPR_2021_paper.pdf' target="_blank"><b>paper</b></a>|<a href='https://github.com/geyuying/PF-AFN' target="_blank"><b>code</b></a>]
		</td>

	</tr>

	<tr>
	<td><center><img width="250" src="./pic/paper/deepfashion2.jpg"></center></td>
	<td>
		<font size="2">DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images,
		<br>
		<i><b>Yuying Ge</b>, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2019
		<br>
		[<a href='https://openaccess.thecvf.com/content_CVPR_2019/papers/Ge_DeepFashion2_A_Versatile_Benchmark_for_Detection_Pose_Estimation_Segmentation_and_CVPR_2019_paper.pdf'  target="_blank"><b>paper</b></a>|<a href='https://github.com/switchablenorms/DeepFashion2' target="_blank"><b>dataset</b></a>]
		</td>
</tbody></table>

